
///////////////////////////////////////////////////////////////////////////////
// Metrics

// Scrape Tempo, Mimir, Loki and itself
// We use the prometheus.scrape component and give this a unique label.
prometheus.scrape "observability" {
    targets = [
        {"__address__" = "load-balancer:9009", group = "observability", service = "mimir"},
        {"__address__" = "tempo:4317", group = "observability", service = "tempo"},
        // {"__address__" = "load-balancer:3100", group = "observability", service = "loki"},
        {"__address__" = "grafana:3000", group = "observability", service = "grafana"},
        {"__address__" = "localhost:12345", group = "observability", service = "alloy"},
    ]

    scrape_interval = "30s"
    forward_to = [prometheus.remote_write.mimir.receiver]
    job_name = "alloy"
}

// This component scrapes the SOA application, defining unique prometheus labels.
prometheus.scrape "soa" {
    // Scrape from the mythical requester and server services, and add them to the 'mythical' group with their service
    // names.
    targets = [
        {"__address__" = "random_service:8071", group = "soa", service = "random_service"},
    ]

    scrape_interval = "30s"
    scrape_timeout = "5s"
    forward_to = [prometheus.remote_write.mimir.receiver]
    job_name = "soa"
}

prometheus.remote_write "mimir" {
    endpoint {
        url = "http://load-balancer:9009/api/v1/push"
        headers = {
          "X-Scope-OrgID" = "demo",
        }
    }
}

///////////////////////////////////////////////////////////////////////////////
// Logging

loki.source.file "soa" {
    targets    = [
        {__path__ = "/tmp/log/app.log"},
    ]
    forward_to = [loki.process.soa_random_service.receiver]
}

loki.process "soa_random_service" {
  forward_to = [loki.write.loki.receiver]

  stage.static_labels {
    values = {
      job = "random_service",
    }
  }
}

loki.write "loki" {
    endpoint {
        url = "http://load-balancer:3100/loki/api/v1/push"
    }
}

///////////////////////////////////////////////////////////////////////////////
// Tracing

otelcol.receiver.otlp "otlp_receiver" {
    grpc {
        endpoint = "0.0.0.0:4317"
    }

    // We define where to send the output of all ingested traces. In this case, to the OpenTelemetry batch processor
    // named 'default'.
    output {
        traces = [
            // The following would be used for tail sampling only traces containing errors.
            // Uncomment the following line, then comment out the line below it (the batch processor) to use
            // tail sampling.
            //otelcol.processor.tail_sampling.errors.input,

            otelcol.processor.batch.default.input,
        ]
    }
}

// The OpenTelemetry batch processor collects trace spans until a batch size or timeout is met, before sending those
// spans onto another target. This processor is labeled 'default'.
otelcol.processor.batch "default" {
    // Wait until we've received 1000 samples, up to a maximum of 2000.
    send_batch_size = 1000
    send_batch_max_size = 2000

    // Or until 2 seconds have elapsed.
    timeout = "2s"

    // When the Alloy has enough batched data, send it to the OpenTelemetry exporter named 'tempo'.
    output {
        traces = [otelcol.exporter.otlp.tempo.input]
    }
}

otelcol.exporter.otlp "tempo" {
    client {
        endpoint = "tempo:4317"

        tls {
            insecure = true
            insecure_skip_verify = true
        }
    }
}

// The Tail Sampling processor will use a set of policies to determine which received traces to keep
// and send to Tempo.
otelcol.processor.tail_sampling "errors" {
    // Total wait time from the start of a trace before making a sampling decision. Note that smaller time
    // periods can potentially cause a decision to be made before the end of a trace has occurred.
    decision_wait = "30s"

    // The following policies follow a logical OR pattern, meaning that if any of the policies match,
    // the trace will be kept. For logical AND, you can use the `and` policy. Every span of a trace is
    // examined by each policy in turn. A match will cause a short-circuit.

    // This policy defines that traces that contain errors should be kept.
    policy {
        // The name of the policy can be used for logging purposes.
        name = "sample-erroring-traces"
        // The type must match the type of policy to be used, in this case examining the status code
        // of every span in the trace.
        type = "status_code"
        // This block determines the error codes that should match in order to keep the trace,
        // in this case the OpenTelemetry 'ERROR' code.
        status_code {
            status_codes = [ "ERROR" ]
        }
    }

    // This policy defines that only traces that are longer than 200ms in total should be kept.
    policy {
        // The name of the policy can be used for logging purposes.
        name = "sample-long-traces"
        // The type must match the policy to be used, in this case the total latency of the trace.
        type = "latency"
        // This block determines the total length of the trace in milliseconds.
        latency {
            threshold_ms = 200
        }
    }

    // The output block forwards the kept traces onto the batch processor, which will marshall them
    // for exporting to Tempo.
    output {
        traces = [otelcol.processor.batch.default.input]
    }
}
